using System;
using System.IO;
using System.Text;
using System.Collections.Generic;
using Tensorflow;
using NumSharp;
using static Tensorflow.Binding;
using SabberStoneUtil.DataProcessing;

namespace SurrogateModel.Surrogate
{
    /// <summary>
    /// Implementation of the fully connected surrogate neural network model.
    /// </summary>
    public class FullyConnectedNN : SurrogateBaseModel
    {
        /// <summary>
        /// Constructor of the model
        /// </summary>
        /// <param name = "num_epoch">Number of epochs to run during training. Default to 10</param>
        /// <param name = "batch_size">Batch size of data. Default to 16</param>
        public FullyConnectedNN(int num_epoch = 10, int batch_size = 64, float step_size = 0.005f)
            : base(num_epoch, batch_size, step_size)
        {
            graph = build_graph();
            sess = tf.Session(config);
            sess.run(init); // initialize the graph
        }

        /// <summary>
        /// Prepare data for training
        /// </summary>
        /// <param name="online">If true, do online training with data generated by DeckSearch, else use locally generated data. Default to false.</param>
        /// <param name = "cardsEncoding">One hot encoded deck data used for training. Used only while online training.</param>
        /// <param name = "deckStats">Target deck data used for training. Used only while online training.</param>
        private void prepare_data(bool online = false, int[,] cardsEncoding = null, double[,] deckStats = null)
        {
            if (!online)
            {
                (cardsEncoding, deckStats) = DataProcessor.PreprocessDeckDataWithOnehotFromFile("resources/individual_log.csv");
            }
            var X = np.array(cardsEncoding);
            X += np.random.rand(X.shape) * 0.0001; // add random noise
            var y = np.array(deckStats);

            int train_test_split = (int)(X.shape[0] * 0.9); // use first 90% of data for training
            // create data loader
            dataLoaderTrain = new DataLoader(X[new Slice(0, train_test_split)],
                                             y[new Slice(0, train_test_split)],
                                             batch_size);
            // dataLoaderTrain = new DataLoader(X[":9000"], y[":9000"], batch_size);
            // regard the last 1000 data points as one batch
            dataLoaderTest = new DataLoader(X[new Slice(train_test_split, X.shape[0])],
                                             y[new Slice(train_test_split, y.Shape[0])],
                                             X.shape[0] - train_test_split, shuffle: false);
            // dataLoaderTest = new DataLoader(X["9000:"], y["9000:"], 1000, shuffle: false);
        }

        /// <summary>
        /// Establish computation graph
        /// </summary>
        private Graph build_graph()
        {
            // creat graph
            var g = tf.get_default_graph();

            // prepare data
            tf_with(tf.variable_scope("placeholder"), delegate
            {
                n_samples = tf.placeholder(tf.float32);
                input = tf.placeholder(tf.float32, shape: (-1, 369));
                y_true = tf.placeholder(tf.float32, shape: (-1, 3));
            });

            // establish graph (architectur of neural net)
            var o_fc1 = fc_layer(input, name: "fc1", num_output: 128);
            var o_acti1 = elu_layer(o_fc1, name: "elu1");

            var o_fc2 = fc_layer(o_acti1, name: "fc2", num_output: 32);
            var o_acti2 = elu_layer(o_fc2, name: "elu2");

            var o_fc3 = fc_layer(o_acti2, name: "fc3", num_output: 16);
            var o_acti3 = elu_layer(o_fc3, name: "elu3");

            var o_fc4 = fc_layer(o_acti3, name: "fc4", num_output: 3);
            model_output = o_fc4;

            // loss
            loss_op = mse_loss(model_output, y_true);

            // optimizer
            var adam =  tf.train.AdamOptimizer(step_size);
            train_op = adam.minimize(loss_op, name: "adam_train");

            init = tf.global_variables_initializer();

            return g;
        }

        /// <summary>
        /// Helper function to write loss to file
        /// </summary>
        private void WriteLosses(List<double> losses, string path)
        {
            using(StreamWriter sw = File.AppendText(path))
            {
                foreach(var loss in losses)
                {
                    sw.WriteLine(loss);
                }
            }
        }

        /// <summary>
        /// offline fit the model using generated data
        /// </summary>
        public void OfflineFit()
        {
            prepare_data();
            train();
        }

        /// <summary>
        /// online fit the model using specified data
        /// </summary>
        public override void OnlineFit(int[,] cardsEncoding, double[,] deckStats)
        {
            prepare_data(online: true, cardsEncoding, deckStats);
            train();
        }

        /// <summary>
        /// Evaluate input, return output. Do not run before initialization
        /// </summary>
        public override double[,] Predict(int[,] cardsEncoding)
        {
            double[,] result;
            var x_input = np.array(cardsEncoding);
            var output = sess.run((model_output), // operations
                                (n_samples, (int)x_input.shape[0]), // batch size
                                (input, x_input)); // features
            // print(output[":10"]);

            // convert result to double array
            result = new double[output.shape[0], output.shape[1]];
            for(int i=0; i<output.shape[0]; i++)
            {
                for(int j=0; j<output.shape[1]; j++)
                {
                    result[i,j] = (double)(float)output[i,j]; // need to cast twice because the model use float
                }
            }
            return result;
        }
    }
}